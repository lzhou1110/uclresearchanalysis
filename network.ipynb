{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calculate': {'analysis': True,\n",
      "               'friends': False,\n",
      "               'network': True,\n",
      "               'uniquetweets': True,\n",
      "               'uniqueusers': True},\n",
      " 'data': {'dates': ['2018-03-11', '2018-03-12', '2018-03-13'],\n",
      "          'eventname': \"Givenchy's Death\",\n",
      "          'phrases': ['givenchy%20death', 'givenchy%20passed%20away'],\n",
      "          'starttime': 'Mar 12 08:20:00 -0500 2018'},\n",
      " 'path': {'cwd': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy',\n",
      "          'ml': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle',\n",
      "          'networkx': {'all': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_all.dat',\n",
      "                       'friends': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_friends.dat',\n",
      "                       'potential': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_potential.dat'},\n",
      "          'newcrawl': '/Users/lzhou/git/github/uclresearchanalysis/other/newcrawl.dat',\n",
      "          'pickle': {'friends': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat',\n",
      "                     'needcrawl': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat',\n",
      "                     'network': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/network.dat',\n",
      "                     'tweets': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat',\n",
      "                     'users': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat'},\n",
      "          'result': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/result',\n",
      "          'twitter': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter'},\n",
      " 'save_to_file': 'False',\n",
      " 'timeframe': '1440'}\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from config import load_tweets_dataframe\n",
    "from config import dump_tweets_dataframe\n",
    "from config import load_users_dataframe\n",
    "from config import dump_users_dataframe\n",
    "from config import load_friends_dictionary\n",
    "from config import dump_friends_dictionary\n",
    "from config import load_needcrawl_set\n",
    "from config import dump_needcrawl_set\n",
    "from config import load_newcrawl_dictionary\n",
    "from config import dump_newcrawl_dictionary\n",
    "from config import dump_networkx_all\n",
    "from config import dump_networkx_friends\n",
    "from config import dump_networkx_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calculate_uniquetweets = config.settings['calculate']['uniquetweets']\n",
    "calculate_uniqueusers = config.settings['calculate']['uniqueusers']\n",
    "calculate_network = config.settings['calculate']['network']\n",
    "calculate_analysis = config.settings['calculate']['analysis']\n",
    "calculate_friends = config.settings['calculate']['friends']\n",
    "\n",
    "file_input_path = config.settings['path']['twitter']\n",
    "dates = config.settings['data']['dates']\n",
    "search_phrases = config.settings['data']['phrases']\n",
    "timeframe = config.settings['timeframe']\n",
    "project_name = config.settings['data']['eventname']\n",
    "starttime = config.settings['data']['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def order_and_reindex(df, column):\n",
    "    df = df.sort_values(by=[column])\n",
    "    df = df.set_index(np.arange(len(df.index)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_utc_to_est(time_string):\n",
    "    datetime_object = dt.datetime.strptime(time_string, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return datetime_object.replace(tzinfo=timezone.utc).astimezone(tz=timezone(-timedelta(hours=5)))\n",
    "\n",
    "def get_created_at(tweet):\n",
    "    return convert_utc_to_est(tweet['created_at'])\n",
    "\n",
    "def get_user_created_days(tweet):\n",
    "    delta_time = convert_utc_to_est(tweet['created_at']) - convert_utc_to_est(tweet['user']['created_at'])\n",
    "    return delta_time.days + 1\n",
    "\n",
    "def get_retweet_id(tweet):\n",
    "    if (tweet['text'].split()[0] == 'RT'):\n",
    "        user_name = tweet['text'].split()[1][1:-1]\n",
    "        mentions = tweet['entities']['user_mentions']\n",
    "        for mention in mentions:\n",
    "            if mention['screen_name'] == user_name:\n",
    "                return string_to_int(mention['id'])\n",
    "\n",
    "def get_reply_id(tweet):\n",
    "    return string_to_int(tweet['in_reply_to_user_id_str'])\n",
    "    \n",
    "def get_user_mentions(tweet):\n",
    "    retweet_id = get_retweet_id(tweet)\n",
    "    reply_id = get_reply_id(tweet)  \n",
    "    mentions = []\n",
    "    for mention in tweet['entities']['user_mentions']:\n",
    "        mention_id = string_to_int(mention['id'])\n",
    "        if mention_id != retweet_id and mention_id != reply_id:\n",
    "            mentions.append(mention_id)\n",
    "    return mentions\n",
    "\n",
    "def string_to_int(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    else:\n",
    "        return int(string)\n",
    "\n",
    "def find_unique_tweets_crawled():\n",
    "    file_path_dict = {\n",
    "        date: ['{}/{}_{}.json'.format(file_input_path, x, date) for x in search_phrases]\n",
    "        for date in dates\n",
    "    }\n",
    "    tweets_crawled_list = []\n",
    "    for date, file_path_list in file_path_dict.items():\n",
    "        for file_path in file_path_list:\n",
    "            if (os.path.isfile(file_path)):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    counter = 0\n",
    "                    for line in file.readlines():\n",
    "                        tweets_crawled_list.append(json.loads(line))\n",
    "                        counter += 1\n",
    "                    print('{}, {}, {}'.format(date, file_path, counter))\n",
    "    \n",
    "    unique_tweets = list({each['id']:each for each in tweets_crawled_list}.values())\n",
    "    start_timestamp = dt.datetime.strptime(starttime, '%b %d %H:%M:%S %z %Y')\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['user'] = list(map(lambda tweet: tweet['user']['screen_name'], unique_tweets))\n",
    "    df['user_statuses_count'] = list(map(lambda tweet: int(tweet['user']['statuses_count']), unique_tweets))\n",
    "    df['user_followers_count'] = list(map(lambda tweet: int(tweet['user']['followers_count']), unique_tweets))\n",
    "    df['user_favourites_count'] = list(map(lambda tweet: int(tweet['user']['favourites_count']), unique_tweets))\n",
    "    df['user_listed_count'] = list(map(lambda tweet: int(tweet['user']['listed_count']), unique_tweets))\n",
    "    df['user_friends_count'] = list(map(lambda tweet: int(tweet['user']['friends_count']), unique_tweets))\n",
    "    df['user_created_days'] = list(map(lambda tweet: get_user_created_days(tweet), unique_tweets))\n",
    "    df['user_id'] = list(map(lambda tweet: string_to_int(tweet['user']['id_str']), unique_tweets))\n",
    "    df['created_at'] = list(map(lambda tweet: get_created_at(tweet), unique_tweets))\n",
    "    df['followers_count'] = list(map(lambda tweet: int(tweet['user']['followers_count']), unique_tweets))\n",
    "    df['friends_count'] = list(map(lambda tweet: int(tweet['user']['friends_count']), unique_tweets))\n",
    "    df['reply_id'] = list(map(lambda tweet: get_reply_id(tweet), unique_tweets))\n",
    "    df['retweet_id'] = list(map(lambda tweet: get_retweet_id(tweet), unique_tweets))\n",
    "    df['at_ids'] = list(map(lambda tweet: get_user_mentions(tweet), unique_tweets))\n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], unique_tweets))\n",
    "    \n",
    "    df = df[df.created_at >= start_timestamp]\n",
    "    \n",
    "    df = order_and_reindex(df, 'created_at')\n",
    "    df['time_lapsed'] = 0\n",
    "    first_tweet_datetime = df.created_at.iloc[0]\n",
    "    for index in tqdm(range(len(df))):\n",
    "        df.loc[index, 'time_lapsed'] = round((df.loc[index, 'created_at'] - first_tweet_datetime).total_seconds() / 60.0, 2)\n",
    "    df = df[df.time_lapsed < float(timeframe)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-11, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-11.json, 0\n",
      "2018-03-11, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-11.json, 0\n",
      "2018-03-12, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-12.json, 216\n",
      "2018-03-12, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-12.json, 4263\n",
      "2018-03-13, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-13.json, 68\n",
      "2018-03-13, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-13.json, 898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5421/5421 [00:03<00:00, 1529.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user  user_statuses_count  user_followers_count  \\\n",
      "0      davelackie               181393                143119   \n",
      "1  alexanderskhan                12853                   753   \n",
      "2   consiglierela                46934                  2161   \n",
      "3  ImpactPrincess                23179                   242   \n",
      "4     Damek0Masca                 2407                   116   \n",
      "\n",
      "   user_favourites_count  user_listed_count  user_friends_count  \\\n",
      "0                  11938                555                4643   \n",
      "1                  34186                  8                3428   \n",
      "2                  78797                 36                 396   \n",
      "3                  28995                 55                 642   \n",
      "4                   1058                 12                 239   \n",
      "\n",
      "   user_created_days     user_id                created_at  followers_count  \\\n",
      "0               2993   100766356 2018-03-12 08:20:58-05:00           143119   \n",
      "1               1042  3186545203 2018-03-12 08:21:07-05:00              753   \n",
      "2                856  4134992843 2018-03-12 08:21:12-05:00             2161   \n",
      "3               1613  1955472014 2018-03-12 08:21:36-05:00              242   \n",
      "4               2318   406301175 2018-03-12 08:22:06-05:00              116   \n",
      "\n",
      "   friends_count  reply_id   retweet_id at_ids  \\\n",
      "0           4643       NaN          NaN     []   \n",
      "1           3428       NaN  100766356.0     []   \n",
      "2            396       NaN  100766356.0     []   \n",
      "3            642       NaN  100766356.0     []   \n",
      "4            239       NaN          NaN     []   \n",
      "\n",
      "                                                text  time_lapsed  \n",
      "0  So sad to hear that fashion designer Hubert de...         0.00  \n",
      "1  RT @davelackie: So sad to hear that fashion de...         0.15  \n",
      "2  RT @davelackie: So sad to hear that fashion de...         0.23  \n",
      "3  RT @davelackie: So sad to hear that fashion de...         0.63  \n",
      "4  Today we mourn the death of a #fashion legend;...         1.13  \n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat\n",
      "'Loaded 5181 entires'\n"
     ]
    }
   ],
   "source": [
    "# if calculate_uniquetweets:\n",
    "unique_tweets = find_unique_tweets_crawled()\n",
    "print(unique_tweets.head())\n",
    "dump_tweets_dataframe(unique_tweets)\n",
    "\n",
    "unique_tweets = load_tweets_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweets['normalized_user_statuses_count'] = np.divide(unique_tweets.user_statuses_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_followers_count'] = np.divide(unique_tweets.user_followers_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_favourites_count'] = np.divide(unique_tweets.user_favourites_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_listed_count'] = np.divide(unique_tweets.user_listed_count, unique_tweets.user_created_days)\n",
    "unique_tweets['normalized_user_friends_count'] = np.divide(unique_tweets.user_friends_count, unique_tweets.user_created_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Friends, and add new crawl relationships if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    }
   ],
   "source": [
    "def merge_new_friends_dictionary():\n",
    "    friends_dictionary = load_friends_dictionary()\n",
    "    newcrawl_dictionary = load_newcrawl_dictionary()\n",
    "    dump_friends_dictionary({**friends_dictionary, **newcrawl_dictionary})\n",
    "\n",
    "if calculate_friends:\n",
    "    merge_new_friends_dictionary()\n",
    "    \n",
    "friends_dictionary = load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_by_user_name(df, user_name):\n",
    "    user = df[df.user == user_name]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_by_user_id(df, user_id):\n",
    "    user = df[df.user_id == user_id]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_index_by_user_id(df, user_id):\n",
    "    return df.user_id[df.user_id == user_id].index.tolist()[0]\n",
    "\n",
    "def find_root_and_generation(df, index):\n",
    "    row = df.iloc[index]\n",
    "    time_lapsed = row.time_lapsed\n",
    "    source_index = row.source_index\n",
    "    generation = int(0)\n",
    "    while source_index is not None:\n",
    "        index = source_index\n",
    "        row = df.iloc[index]\n",
    "        source_index = row.source_index\n",
    "        generation += 1\n",
    "    root_time = row.time_lapsed\n",
    "    return (index, generation, time_lapsed-root_time)\n",
    "\n",
    "def find_unique_users():\n",
    "    df = unique_tweets.copy()\n",
    "    df = df.drop_duplicates(subset = ['user_id'])\n",
    "    columns = [\n",
    "        'user', 'user_id', 'time_lapsed', 'followers_count', 'friends_count',\n",
    "        'user_created_days','user_statuses_count','user_listed_count','user_favourites_count',\n",
    "        'normalized_user_statuses_count', 'normalized_user_followers_count',\n",
    "        'normalized_user_favourites_count', 'normalized_user_listed_count', 'normalized_user_friends_count'\n",
    "    ]\n",
    "    \n",
    "    df = df.loc[:,columns]\n",
    "    df['mention_and_reply'] = [[] for _ in range(len(df))]\n",
    "    df['source_candidates'] = [[] for _ in range(len(df))]\n",
    "    df['source_index'] = [None for _ in range(len(df))]\n",
    "    df['seed_index'] = [None for _ in range(len(df))]\n",
    "    df['generation'] = [None for _ in range(len(df))]\n",
    "    df['time_since_seed'] = [None for _ in range(len(df))]\n",
    "    \n",
    "    df = order_and_reindex(df, 'time_lapsed')\n",
    "    \n",
    "    unique_user_id_set = set([int(x) for x in df.user_id])\n",
    "    \n",
    "    for index in tqdm(range(len(unique_tweets))):\n",
    "        user_name = unique_tweets.loc[index, 'user']\n",
    "        user_id = unique_tweets.loc[index, 'user_id']\n",
    "        reply_id = unique_tweets.loc[index, 'reply_id']\n",
    "        retweet_id = unique_tweets.loc[index, 'retweet_id']\n",
    "        at_ids = unique_tweets.loc[index, 'at_ids']\n",
    "        \n",
    "        if reply_id is not None:\n",
    "            if reply_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).mention_and_reply.append(find_index_by_user_id(df, int(reply_id)))\n",
    "                except:\n",
    "                    pass\n",
    "        if retweet_id is not None:\n",
    "            if retweet_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).mention_and_reply.append(find_index_by_user_id(df, int(retweet_id)))\n",
    "                except:\n",
    "                    pass\n",
    "        for at_id in at_ids:\n",
    "            if at_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, at_id).mention_and_reply.append(find_index_by_user_id(df, int(user_id)))\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "    friends_not_found_list = []\n",
    "    for index in tqdm(range(len(df))):\n",
    "        user_id = str(df.loc[index, 'user_id'])\n",
    "        try:\n",
    "            friends = (set(friends_dictionary[int(user_id)]) & unique_user_id_set)\n",
    "            friends_indexes = [find_index_by_user_id(df, x) for x in friends]\n",
    "            friends_indexes.extend(df.loc[index, 'mention_and_reply'])\n",
    "            friends_indexes = sorted([x for x in set(friends_indexes)])\n",
    "            df.loc[index, 'source_candidates'].extend(friends_indexes)\n",
    "            if len(friends_indexes) > 0:\n",
    "                if (friends_indexes[0] < index):\n",
    "                    df.loc[index, 'source_index'] = friends_indexes[0]\n",
    "            df.loc[index, 'seed_index'], df.loc[index, 'generation'], df.loc[index, 'time_since_seed'] = find_root_and_generation(df, index)\n",
    "        except KeyError:\n",
    "            friends_not_found_list.append(index)\n",
    "            \n",
    "    print('Could not load friends for {}/{} entries'.format(len(friends_not_found_list), len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5181/5181 [00:06<00:00, 751.36it/s]\n",
      "100%|██████████| 5011/5011 [00:27<00:00, 183.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load friends for 0/5011 entries\n",
      "             user     user_id  time_lapsed  followers_count  friends_count  \\\n",
      "0      davelackie   100766356         0.00           143119           4643   \n",
      "1  alexanderskhan  3186545203         0.15              753           3428   \n",
      "2   consiglierela  4134992843         0.23             2161            396   \n",
      "3  ImpactPrincess  1955472014         0.63              242            642   \n",
      "4     Damek0Masca   406301175         1.13              116            239   \n",
      "\n",
      "   user_created_days  user_statuses_count  user_listed_count  \\\n",
      "0               2993               181393                555   \n",
      "1               1042                12853                  8   \n",
      "2                856                46934                 36   \n",
      "3               1613                23179                 55   \n",
      "4               2318                 2407                 12   \n",
      "\n",
      "   user_favourites_count  normalized_user_statuses_count  \\\n",
      "0                  11938                       60.605747   \n",
      "1                  34186                       12.334933   \n",
      "2                  78797                       54.829439   \n",
      "3                  28995                       14.370118   \n",
      "4                   1058                        1.038395   \n",
      "\n",
      "   normalized_user_followers_count  normalized_user_favourites_count  \\\n",
      "0                        47.817908                          3.988640   \n",
      "1                         0.722649                         32.808061   \n",
      "2                         2.524533                         92.052570   \n",
      "3                         0.150031                         17.975821   \n",
      "4                         0.050043                          0.456428   \n",
      "\n",
      "   normalized_user_listed_count  normalized_user_friends_count  \\\n",
      "0                      0.185433                       1.551286   \n",
      "1                      0.007678                       3.289827   \n",
      "2                      0.042056                       0.462617   \n",
      "3                      0.034098                       0.398016   \n",
      "4                      0.005177                       0.103106   \n",
      "\n",
      "  mention_and_reply                                  source_candidates  \\\n",
      "0                []  [3, 5, 6, 7, 10, 11, 14, 15, 19, 20, 24, 74, 8...   \n",
      "1               [0]                                                [0]   \n",
      "2               [0]  [0, 12, 21, 32, 48, 748, 819, 972, 2358, 2778,...   \n",
      "3               [0]                                      [0, 115, 577]   \n",
      "4                []                                                 []   \n",
      "\n",
      "  source_index  seed_index  generation time_since_seed  \n",
      "0         None           0           0               0  \n",
      "1            0           0           1            0.15  \n",
      "2            0           0           1            0.23  \n",
      "3            0           0           1            0.63  \n",
      "4         None           4           0               0  \n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    }
   ],
   "source": [
    "if calculate_uniqueusers:\n",
    "    unique_users = find_unique_users()\n",
    "    print(unique_users.head())\n",
    "    dump_users_dataframe(unique_users)\n",
    "unique_users = load_users_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Friends Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_friends_dictionary():\n",
    "    friends_dictionary = load_friends_dictionary()\n",
    "    crawled_set = set(friends_dictionary.keys())\n",
    "    users_set = set(unique_users.user_id)\n",
    "    need_to_crawl = users_set - crawled_set\n",
    "    dump_needcrawl_set(need_to_crawl)\n",
    "    print('Number of users still need to crawl: {}'.format(len(need_to_crawl)))   \n",
    "    \n",
    "    unwanted = set(crawled_set) - set(users_set)\n",
    "    for unwanted_key in unwanted:\n",
    "        del friends_dictionary[unwanted_key]\n",
    "    dump_friends_dictionary(friends_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat')\n",
      "Number of users still need to crawl: 0\n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    }
   ],
   "source": [
    "verify_friends_dictionary()\n",
    "friends_dictionary = load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gexf(network, 'givenchy_network.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5011/5011 [00:00<00:00, 12755.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_all.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_all.dat')\n"
     ]
    }
   ],
   "source": [
    "network_all = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_all.add_node(index,\n",
    "                         user = unique_users.loc[index, 'user'],\n",
    "                         user_id = unique_users.loc[index, 'user_id'],\n",
    "                         time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                         followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                         friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                         generation = unique_users.loc[index, 'generation'],\n",
    "                         time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    source_index = unique_users.loc[index, 'source_index']\n",
    "    if source_index is not None:\n",
    "        network_all.add_edge(source_index, index)\n",
    "\n",
    "dump_networkx_all(network_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 21, 748, 1912, 1929]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users.iloc[6].source_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5011/5011 [00:01<00:00, 3433.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_friends.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_friends.dat')\n"
     ]
    }
   ],
   "source": [
    "network_friends = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_friends.add_node(index,\n",
    "                             user = unique_users.loc[index, 'user'],\n",
    "                             user_id = unique_users.loc[index, 'user_id'],\n",
    "                             time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                             followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                             friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                             generation = unique_users.loc[index, 'generation'],\n",
    "                             time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                            )\n",
    "    source_candidates = unique_users.iloc[index].source_candidates\n",
    "    for source_index in source_candidates:\n",
    "        network_friends.add_edge(source_index, index)\n",
    "dump_networkx_friends(network_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5011/5011 [00:01<00:00, 3581.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_potential.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/networkx_potential.dat')\n"
     ]
    }
   ],
   "source": [
    "network_potential = nx.DiGraph()\n",
    "for index in tqdm(range(len(unique_users))):\n",
    "    network_potential.add_node(index,\n",
    "                             user = unique_users.loc[index, 'user'],\n",
    "                             user_id = unique_users.loc[index, 'user_id'],\n",
    "                             time_lapsed = unique_users.loc[index, 'time_lapsed'],\n",
    "                             followers_count = unique_users.loc[index, 'followers_count'],\n",
    "                             friends_count = unique_users.loc[index, 'friends_count'],\n",
    "                             generation = unique_users.loc[index, 'generation'],\n",
    "                             time_since_seed = unique_users.loc[index, 'time_since_seed'],\n",
    "                            )\n",
    "    source_candidates = unique_users.iloc[index].source_candidates\n",
    "    for source_index in source_candidates:\n",
    "        if source_index < index:\n",
    "            network_potential.add_edge(source_index, index)\n",
    "dump_networkx_potential(network_potential)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
