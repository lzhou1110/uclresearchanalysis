{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'dates': ['2018-03-11', '2018-03-12', '2018-03-13'],\n",
      "          'phrases': ['givenchy%20death', 'givenchy%20passed%20away'],\n",
      "          'time': {'end': 'Mar 13 00:00:00 -0500 2018',\n",
      "                   'start': 'Mar 12 00:00:00 -0500 2018'}},\n",
      " 'path': {'cwd': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy',\n",
      "          'friends_dictionary': '/Users/lzhou/git/github/uclresearchanalysis/data/friends_dictionary.dat',\n",
      "          'pickle': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle',\n",
      "          'result': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/result',\n",
      "          'twitter': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter'}}\n"
     ]
    }
   ],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load city and state dictionaries from dat files\n",
      "loaded 102 states abbrev, loaded 51 states, loaded 2361 cities\n"
     ]
    }
   ],
   "source": [
    "from config import load_us_city_state_files\n",
    "city_to_state_dict, abbrev_to_state_dict, state_to_state_dict = load_us_city_state_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_input_path = config.settings['path']['twitter']\n",
    "file_output_path = config.settings['path']['pickle']\n",
    "dates = config.settings['data']['dates']\n",
    "search_phrases = config.settings['data']['phrases']\n",
    "start_time = config.settings['data']['time']['start']\n",
    "end_time = config.settings['data']['time']['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_tweets_in_project():\n",
    "    file_path_dict = {date:['{}/{}_{}.json'.format(file_input_path, x, date) for x in search_phrases] for date in dates}\n",
    "    all_tweets_collected = []\n",
    "    for date, file_path_list in file_path_dict.items():\n",
    "        tweet_counter_date = 0\n",
    "        print('Processing files for date: {}'.format(date))\n",
    "        for file_path in file_path_list:\n",
    "            tweet_counter_file = 0\n",
    "            if (os.path.isfile(file_path)):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file.readlines():\n",
    "                        all_tweets_collected.append(json.loads(line))\n",
    "                        tweet_counter_file = tweet_counter_file + 1\n",
    "            tweet_counter_date = tweet_counter_date + tweet_counter_file\n",
    "            print('Finished processing file: {}'.format(file_path))\n",
    "            print('Found {} tweets'.format(tweet_counter_file))\n",
    "        print('In total, found {} tweets on date {}'.format(tweet_counter_date, date))\n",
    "    all_unique_tweets = list({each['id']:each for each in all_tweets_collected}.values())\n",
    "    print('In total, found {} unique tweets'.format(len(all_unique_tweets)))\n",
    "    return all_unique_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utc_to_est(tweet_created_at):\n",
    "    datetime_object = dt.datetime.strptime(tweet_created_at, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return datetime_object.replace(tzinfo=timezone.utc).astimezone(tz=timezone(-timedelta(hours=5)))\n",
    "\n",
    "def is_retweet(tweet):\n",
    "    if (tweet['text'].split()[0] == 'RT'):\n",
    "        return tweet['text'].split()[1][1:-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_reply(tweet):\n",
    "    return tweet['in_reply_to_user_id_str'] != None\n",
    "\n",
    "def get_source(tweet):\n",
    "    if (tweet['text'].split()[0] == 'RT'):\n",
    "        return 'RT'\n",
    "    if (tweet['in_reply_to_user_id_str'] != None):\n",
    "        return 'RP'\n",
    "    return ''\n",
    "\n",
    "def get_source_user(tweet):\n",
    "    source = get_source(tweet)\n",
    "    if (source == 'RT'):\n",
    "        return tweet['text'].split()[1][1:-1]\n",
    "    return ''\n",
    "\n",
    "def get_source_user_id(tweet):\n",
    "    source = get_source(tweet)\n",
    "    if (source == 'RP'):\n",
    "        return str(tweet['in_reply_to_user_id'])\n",
    "    return ''\n",
    "\n",
    "def get_user_mentions(tweet):\n",
    "    mentions = []\n",
    "    for mention in tweet['entities']['user_mentions']:\n",
    "        mentions.append(mention['screen_name'])\n",
    "    return mentions\n",
    "\n",
    "def populate_tweet_df(tweets):\n",
    "    df = pd.DataFrame()\n",
    "    df['user'] = list(map(lambda tweet: tweet['user']['screen_name'], tweets))\n",
    "    df['user_id'] = list(map(lambda tweet: tweet['user']['id_str'], tweets))\n",
    "    df['created_at'] = list(map(lambda tweet: convert_utc_to_est(tweet['created_at']), tweets))\n",
    "    df['source_type'] = list(map(lambda tweet: get_source(tweet), tweets))\n",
    "    df['source_user'] = list(map(lambda tweet: get_source_user(tweet), tweets))\n",
    "    df['source_user_id'] = list(map(lambda tweet: get_source_user_id(tweet), tweets))\n",
    "    df['seed_user'] = ''\n",
    "    df['seed_user_id'] = ''\n",
    "    df['followers_count'] = list(map(lambda tweet: tweet['user']['followers_count'], tweets))\n",
    "    df['friends_count'] = list(map(lambda tweet: tweet['user']['friends_count'], tweets))\n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], tweets))\n",
    "    df['location'] = list(map(lambda tweet: tweet['user']['location'], tweets))\n",
    "    df['country_code'] = list(map(lambda tweet: tweet['place']['country_code'] if tweet['place'] != None else '', tweets))\n",
    "    df['long'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][0] if tweet['coordinates'] != None else 'NaN', tweets))\n",
    "    df['latt'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][1] if tweet['coordinates'] != None else 'NaN', tweets))\n",
    "    df['state'] = ''\n",
    "    df['mentions'] = list(map(lambda tweet: get_user_mentions(tweet), tweets))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE)\n",
    "def find_word_case(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w))\n",
    "\n",
    "abbrev_to_state_regex_dict = {key:find_word_case(key) for key in abbrev_to_state_dict.keys()}\n",
    "state_to_state_regex_dict = {key:find_word(key) for key in state_to_state_dict.keys()}\n",
    "city_to_state_regex_dict = {key:find_word(key) for key in city_to_state_dict.keys()}\n",
    "\n",
    "def find_state_name(tweet_location):\n",
    "    # look for the state ID in\n",
    "    # the location string\n",
    "    for key, regex in abbrev_to_state_regex_dict.items():\n",
    "        if regex.search(tweet_location):\n",
    "            return abbrev_to_state_dict[key]\n",
    "    # otherwise look for states\n",
    "    for key, regex in state_to_state_regex_dict.items():\n",
    "        if regex.search(tweet_location):\n",
    "            return state_to_state_dict[key]\n",
    "    # otherwise look for cities\n",
    "    for key, regex in city_to_state_regex_dict.items():\n",
    "        if regex.search(tweet_location):\n",
    "            return city_to_state_dict[key]\n",
    "    # otherwise return empty string\n",
    "    return ''\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    num_cores = mp.cpu_count()\n",
    "    num_partitions = num_cores\n",
    "    print('Found {} CPUs, will split tasks into {} partitions'.format(num_cores, num_partitions))\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def adding_state(data):\n",
    "    for i in data.index:\n",
    "        name = find_state_name(data.location[i])\n",
    "        data.loc[i, 'state'] = name\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_dates_and_reindex(df):\n",
    "    # The bomb attack happened at around 7:20 am\n",
    "    # We are interested in tweets after 7:00 am\n",
    "    timeline_start = dt.datetime.strptime(start_time, '%b %d %H:%M:%S %z %Y')\n",
    "    timeline_end = dt.datetime.strptime(end_time, '%b %d %H:%M:%S %z %Y')\n",
    "    df = df[(df.created_at >= timeline_start) & (df.created_at <= timeline_end)]\n",
    "    df = df.sort_values(by=['created_at'])\n",
    "    df = df.set_index(np.arange(len(df.index)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files for date: 2018-03-11\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-11.json\n",
      "Found 0 tweets\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-11.json\n",
      "Found 0 tweets\n",
      "In total, found 0 tweets on date 2018-03-11\n",
      "Processing files for date: 2018-03-12\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-12.json\n",
      "Found 216 tweets\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-12.json\n",
      "Found 4263 tweets\n",
      "In total, found 4479 tweets on date 2018-03-12\n",
      "Processing files for date: 2018-03-13\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-13.json\n",
      "Found 68 tweets\n",
      "Finished processing file: /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-13.json\n",
      "Found 898 tweets\n",
      "In total, found 966 tweets on date 2018-03-13\n",
      "In total, found 5421 unique tweets\n",
      "Found 4 CPUs, will split tasks into 4 partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.80k/4.80k [00:46<00:00, 106Tweets/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicate users: 158\n",
      "4754 users left after removing duplicated users\n"
     ]
    }
   ],
   "source": [
    "all_unique_tweets = find_unique_tweets_in_project()\n",
    "df = populate_tweet_df(all_unique_tweets)\n",
    "df = parallelize_dataframe(df, adding_state)\n",
    "df = order_by_dates_and_reindex(df)\n",
    "user_list = []\n",
    "index = 0\n",
    "with tqdm(total = len(df), unit='Tweets', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "    while index < len(df):\n",
    "        mentions = df.loc[index, 'mentions']\n",
    "        for mention in mentions:\n",
    "            df.loc[(df['user'] == mention) & (df.index.values > index), 'source_type'] = 'AT'\n",
    "            df.loc[(df['user'] == mention) & (df.index.values > index), 'source_user'] = mention\n",
    "        user_id = df.loc[index, 'user_id']\n",
    "        if user_id in user_list:\n",
    "            df.loc[index, 'source_type'] = 'Duplicated'\n",
    "        user_list.append(user_id)\n",
    "        index += 1\n",
    "        pbar.update()\n",
    "df = order_by_dates_and_reindex(df)  \n",
    "print('Found duplicate users: {}'.format(len(df[df.source_type == 'Duplicated'])))\n",
    "df = df[df.source_type != 'Duplicated']\n",
    "df = order_by_dates_and_reindex(df)\n",
    "print('{} users left after removing duplicated users'.format(len(df)))\n",
    "with open(file_output_path + '/user_unique.dat', 'wb') as data_file:\n",
    "    pickle.dump(df, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_user</th>\n",
       "      <th>source_user_id</th>\n",
       "      <th>seed_user</th>\n",
       "      <th>seed_user_id</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>country_code</th>\n",
       "      <th>long</th>\n",
       "      <th>latt</th>\n",
       "      <th>state</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>davelackie</td>\n",
       "      <td>100766356</td>\n",
       "      <td>2018-03-12 08:20:58-05:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>143119</td>\n",
       "      <td>4643</td>\n",
       "      <td>So sad to hear that fashion designer Hubert de...</td>\n",
       "      <td>Canada</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alexanderskhan</td>\n",
       "      <td>3186545203</td>\n",
       "      <td>2018-03-12 08:21:07-05:00</td>\n",
       "      <td>RT</td>\n",
       "      <td>davelackie</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>753</td>\n",
       "      <td>3428</td>\n",
       "      <td>RT @davelackie: So sad to hear that fashion de...</td>\n",
       "      <td>New Orleans, LA</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>[davelackie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consiglierela</td>\n",
       "      <td>4134992843</td>\n",
       "      <td>2018-03-12 08:21:12-05:00</td>\n",
       "      <td>RT</td>\n",
       "      <td>davelackie</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2161</td>\n",
       "      <td>396</td>\n",
       "      <td>RT @davelackie: So sad to hear that fashion de...</td>\n",
       "      <td>⭐</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>[davelackie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ImpactPrincess</td>\n",
       "      <td>1955472014</td>\n",
       "      <td>2018-03-12 08:21:36-05:00</td>\n",
       "      <td>RT</td>\n",
       "      <td>davelackie</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>242</td>\n",
       "      <td>642</td>\n",
       "      <td>RT @davelackie: So sad to hear that fashion de...</td>\n",
       "      <td>David Jones, Probably</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>[davelackie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Damek0Masca</td>\n",
       "      <td>406301175</td>\n",
       "      <td>2018-03-12 08:22:06-05:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>116</td>\n",
       "      <td>239</td>\n",
       "      <td>Today we mourn the death of a #fashion legend;...</td>\n",
       "      <td>New York</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user     user_id                created_at source_type  \\\n",
       "0      davelackie   100766356 2018-03-12 08:20:58-05:00               \n",
       "1  alexanderskhan  3186545203 2018-03-12 08:21:07-05:00          RT   \n",
       "2   consiglierela  4134992843 2018-03-12 08:21:12-05:00          RT   \n",
       "3  ImpactPrincess  1955472014 2018-03-12 08:21:36-05:00          RT   \n",
       "4     Damek0Masca   406301175 2018-03-12 08:22:06-05:00               \n",
       "\n",
       "  source_user source_user_id seed_user seed_user_id  followers_count  \\\n",
       "0                                                             143119   \n",
       "1  davelackie                                                    753   \n",
       "2  davelackie                                                   2161   \n",
       "3  davelackie                                                    242   \n",
       "4                                                                116   \n",
       "\n",
       "   friends_count                                               text  \\\n",
       "0           4643  So sad to hear that fashion designer Hubert de...   \n",
       "1           3428  RT @davelackie: So sad to hear that fashion de...   \n",
       "2            396  RT @davelackie: So sad to hear that fashion de...   \n",
       "3            642  RT @davelackie: So sad to hear that fashion de...   \n",
       "4            239  Today we mourn the death of a #fashion legend;...   \n",
       "\n",
       "                location country_code long latt      state      mentions  \n",
       "0                 Canada               NaN  NaN                       []  \n",
       "1        New Orleans, LA               NaN  NaN  Louisiana  [davelackie]  \n",
       "2                      ⭐               NaN  NaN             [davelackie]  \n",
       "3  David Jones, Probably               NaN  NaN             [davelackie]  \n",
       "4               New York               NaN  NaN   New York            []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
