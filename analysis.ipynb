{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calculate': {'analysis': True,\n",
      "               'friends': False,\n",
      "               'network': True,\n",
      "               'uniquetweets': True,\n",
      "               'uniqueusers': True},\n",
      " 'data': {'dates': ['2018-03-11', '2018-03-12', '2018-03-13'],\n",
      "          'eventname': \"givenchy's death\",\n",
      "          'phrases': ['givenchy%20death', 'givenchy%20passed%20away'],\n",
      "          'starttime': 'Mar 12 08:20:00 -0500 2018'},\n",
      " 'path': {'cwd': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy',\n",
      "          'newcrawl': '/Users/lzhou/git/github/uclresearchanalysis/other/newcrawl.dat',\n",
      "          'pickle': {'friends': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat',\n",
      "                     'needcrawl': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat',\n",
      "                     'network': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/network.dat',\n",
      "                     'tweets': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat',\n",
      "                     'users': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat'},\n",
      "          'result': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/result',\n",
      "          'twitter': '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter'},\n",
      " 'timeframe': '1440'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 9]\n",
    "\n",
    "import config\n",
    "from config import load_tweets_dataframe\n",
    "from config import dump_tweets_dataframe\n",
    "from config import load_users_dataframe\n",
    "from config import dump_users_dataframe\n",
    "from config import load_network_dataframe\n",
    "from config import dump_network_dataframe\n",
    "from config import load_friends_dictionary\n",
    "from config import dump_friends_dictionary\n",
    "from config import load_needcrawl_set\n",
    "from config import dump_needcrawl_set\n",
    "from config import load_newcrawl_dictionary\n",
    "from config import dump_newcrawl_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_uniquetweets = config.settings['calculate']['uniquetweets']\n",
    "calculate_uniqueusers = config.settings['calculate']['uniqueusers']\n",
    "calculate_network = config.settings['calculate']['network']\n",
    "calculate_analysis = config.settings['calculate']['analysis']\n",
    "calculate_friends = config.settings['calculate']['friends']\n",
    "\n",
    "file_input_path = config.settings['path']['twitter']\n",
    "dates = config.settings['data']['dates']\n",
    "search_phrases = config.settings['data']['phrases']\n",
    "timeframe = config.settings['timeframe']\n",
    "project_name = config.settings['data']['eventname']\n",
    "starttime = config.settings['data']['starttime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_reindex(df, column):\n",
    "    df = df.sort_values(by=[column])\n",
    "    df = df.set_index(np.arange(len(df.index)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utc_to_est(time_string):\n",
    "    datetime_object = dt.datetime.strptime(time_string, '%a %b %d %H:%M:%S %z %Y')\n",
    "    return datetime_object.replace(tzinfo=timezone.utc).astimezone(tz=timezone(-timedelta(hours=5)))\n",
    "\n",
    "def get_created_at(tweet):\n",
    "    return convert_utc_to_est(tweet['created_at'])\n",
    "\n",
    "def get_retweet_id(tweet):\n",
    "    if (tweet['text'].split()[0] == 'RT'):\n",
    "        user_name = tweet['text'].split()[1][1:-1]\n",
    "        mentions = tweet['entities']['user_mentions']\n",
    "        for mention in mentions:\n",
    "            if mention['screen_name'] == user_name:\n",
    "                return string_to_int(mention['id'])\n",
    "\n",
    "def get_reply_id(tweet):\n",
    "    return string_to_int(tweet['in_reply_to_user_id_str'])\n",
    "    \n",
    "def get_user_mentions(tweet):\n",
    "    retweet_id = get_retweet_id(tweet)\n",
    "    reply_id = get_reply_id(tweet)  \n",
    "    mentions = []\n",
    "    for mention in tweet['entities']['user_mentions']:\n",
    "        mention_id = string_to_int(mention['id'])\n",
    "        if mention_id != retweet_id and mention_id != reply_id:\n",
    "            mentions.append(mention_id)\n",
    "    return mentions\n",
    "\n",
    "def string_to_int(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    else:\n",
    "        return int(string)\n",
    "\n",
    "def find_unique_tweets_crawled():\n",
    "    file_path_dict = {\n",
    "        date: ['{}/{}_{}.json'.format(file_input_path, x, date) for x in search_phrases]\n",
    "        for date in dates\n",
    "    }\n",
    "    tweets_crawled_list = []\n",
    "    for date, file_path_list in file_path_dict.items():\n",
    "        for file_path in file_path_list:\n",
    "            if (os.path.isfile(file_path)):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    counter = 0\n",
    "                    for line in file.readlines():\n",
    "                        tweets_crawled_list.append(json.loads(line))\n",
    "                        counter += 1\n",
    "                    print('{}, {}, {}'.format(date, file_path, counter))\n",
    "    \n",
    "    unique_tweets = list({each['id']:each for each in tweets_crawled_list}.values())\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['user'] = list(map(lambda tweet: tweet['user']['screen_name'], unique_tweets))\n",
    "    df['user_id'] = list(map(lambda tweet: string_to_int(tweet['user']['id_str']), unique_tweets))\n",
    "    df['created_at'] = list(map(lambda tweet: get_created_at(tweet), unique_tweets))\n",
    "    df['followers_count'] = list(map(lambda tweet: int(tweet['user']['followers_count']), unique_tweets))\n",
    "    df['friends_count'] = list(map(lambda tweet: int(tweet['user']['friends_count']), unique_tweets))\n",
    "    df['reply_id'] = list(map(lambda tweet: get_reply_id(tweet), unique_tweets))\n",
    "    df['retweet_id'] = list(map(lambda tweet: get_retweet_id(tweet), unique_tweets))\n",
    "    df['at_ids'] = list(map(lambda tweet: get_user_mentions(tweet), unique_tweets))\n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], unique_tweets))\n",
    "    \n",
    "    df = df[df.created_at >= dt.datetime.strptime(starttime, '%b %d %H:%M:%S %z %Y')]\n",
    "    \n",
    "    df = order_and_reindex(df, 'created_at')\n",
    "    df['time'] = 0\n",
    "    first_tweet_datetime = df.created_at.iloc[0]\n",
    "    for index in tqdm(range(len(df))):\n",
    "        df.loc[index, 'time'] = round((df.loc[index, 'created_at'] - first_tweet_datetime).total_seconds() / 60.0, 2)\n",
    "    df = df[df.time < float(timeframe)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-11, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-11.json, 0\n",
      "2018-03-11, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-11.json, 0\n",
      "2018-03-12, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-12.json, 216\n",
      "2018-03-12, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-12.json, 4263\n",
      "2018-03-13, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20death_2018-03-13.json, 68\n",
      "2018-03-13, /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/twitter/givenchy%20passed%20away_2018-03-13.json, 898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5421/5421 [00:03<00:00, 1599.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user     user_id                created_at  followers_count  \\\n",
      "0      davelackie   100766356 2018-03-12 08:20:58-05:00           143119   \n",
      "1  alexanderskhan  3186545203 2018-03-12 08:21:07-05:00              753   \n",
      "2   consiglierela  4134992843 2018-03-12 08:21:12-05:00             2161   \n",
      "3  ImpactPrincess  1955472014 2018-03-12 08:21:36-05:00              242   \n",
      "4     Damek0Masca   406301175 2018-03-12 08:22:06-05:00              116   \n",
      "\n",
      "   friends_count  reply_id   retweet_id at_ids  \\\n",
      "0           4643       NaN          NaN     []   \n",
      "1           3428       NaN  100766356.0     []   \n",
      "2            396       NaN  100766356.0     []   \n",
      "3            642       NaN  100766356.0     []   \n",
      "4            239       NaN          NaN     []   \n",
      "\n",
      "                                                text  time  \n",
      "0  So sad to hear that fashion designer Hubert de...  0.00  \n",
      "1  RT @davelackie: So sad to hear that fashion de...  0.15  \n",
      "2  RT @davelackie: So sad to hear that fashion de...  0.23  \n",
      "3  RT @davelackie: So sad to hear that fashion de...  0.63  \n",
      "4  Today we mourn the death of a #fashion legend;...  1.13  \n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/tweets.dat\n",
      "'Loaded 5181 entires'\n"
     ]
    }
   ],
   "source": [
    "if calculate_uniquetweets:\n",
    "    unique_tweets = find_unique_tweets_crawled()\n",
    "    print(unique_tweets.head())\n",
    "    dump_tweets_dataframe(unique_tweets)\n",
    "\n",
    "unique_tweets = load_tweets_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Friends, and add new crawl relationships if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    }
   ],
   "source": [
    "def merge_new_friends_dictionary():\n",
    "    friends_dictionary = load_friends_dictionary()\n",
    "    newcrawl_dictionary = load_newcrawl_dictionary()\n",
    "    dump_friends_dictionary({**friends_dictionary, **newcrawl_dictionary})\n",
    "\n",
    "if calculate_friends:\n",
    "    merge_new_friends_dictionary()\n",
    "    \n",
    "friends_dictionary = load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_by_user_name(df, user_name):\n",
    "    user = df[df.user == user_name]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_by_user_id(df, user_id):\n",
    "    user = df[df.user_id == user_id]\n",
    "    return user.iloc[0]\n",
    "\n",
    "def find_unique_users():\n",
    "    df = unique_tweets.copy()\n",
    "    df = df.drop_duplicates(subset = ['user_id'])\n",
    "    df = df.loc[:,['user', 'user_id', 'time', 'followers_count', 'friends_count']]\n",
    "    df['source_candidates'] = [set([]) for _ in range(len(df))]\n",
    "    df = order_and_reindex(df, 'time')\n",
    "    \n",
    "    unique_user_id_set = set([int(x) for x in df.user_id])\n",
    "    \n",
    "    for index in tqdm(range(len(unique_tweets))):\n",
    "        user_name = unique_tweets.loc[index, 'user']\n",
    "        user_id = unique_tweets.loc[index, 'user_id']\n",
    "        reply_id = unique_tweets.loc[index, 'reply_id']\n",
    "        retweet_id = unique_tweets.loc[index, 'retweet_id']\n",
    "        at_ids = unique_tweets.loc[index, 'at_ids']\n",
    "        \n",
    "        if reply_id is not None:\n",
    "            if reply_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).source_candidates.add(int(reply_id))\n",
    "                except:\n",
    "                    pass\n",
    "        if retweet_id is not None:\n",
    "            if retweet_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, user_id).source_candidates.add(int(retweet_id))\n",
    "                except:\n",
    "                    pass\n",
    "        for at_id in at_ids:\n",
    "            if at_id in unique_user_id_set:\n",
    "                try:\n",
    "                    find_by_user_id(df, at_id).source_candidates.add(int(user_id))\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "    friends_not_found_list = []\n",
    "    for index in tqdm(range(len(df))):\n",
    "        user_id = str(df.loc[index, 'user_id'])\n",
    "        try:\n",
    "            friends = set(friends_dictionary[user_id]) & unique_user_id_set\n",
    "            df.loc[index, 'source_candidates'].update(friends)\n",
    "        except KeyError:\n",
    "            friends_not_found_list.append(index)\n",
    "    print('Could not load friends for {}/{} entries'.format(len(friends_not_found_list), len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5181/5181 [00:04<00:00, 1103.15it/s]\n",
      "100%|██████████| 5011/5011 [00:00<00:00, 89271.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load friends for 5011/5011 entries\n",
      "             user     user_id  time  followers_count  friends_count  \\\n",
      "0      davelackie   100766356  0.00           143119           4643   \n",
      "1  alexanderskhan  3186545203  0.15              753           3428   \n",
      "2   consiglierela  4134992843  0.23             2161            396   \n",
      "3  ImpactPrincess  1955472014  0.63              242            642   \n",
      "4     Damek0Masca   406301175  1.13              116            239   \n",
      "\n",
      "  source_candidates  \n",
      "0                {}  \n",
      "1       {100766356}  \n",
      "2       {100766356}  \n",
      "3       {100766356}  \n",
      "4                {}  \n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/users.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if calculate_uniqueusers:\n",
    "    unique_users = find_unique_users()\n",
    "    print(unique_users.head())\n",
    "    dump_users_dataframe(unique_users)\n",
    "\n",
    "unique_users = load_users_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Friends Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_friends_dictionary():\n",
    "    friends_dictionary = load_friends_dictionary()\n",
    "    crawled_set = set(friends_dictionary.keys())\n",
    "    users_set = set(unique_users.user_id)\n",
    "    need_to_crawl = users_set - crawled_set\n",
    "    dump_needcrawl_set(need_to_crawl)\n",
    "    print('Number of users still need to crawl: {}'.format(len(need_to_crawl)))   \n",
    "    \n",
    "    unwanted = set(crawled_set) - set(users_set)\n",
    "    for unwanted_key in unwanted:\n",
    "        del friends_dictionary[unwanted_key]\n",
    "    dump_friends_dictionary(friends_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/needcrawl.dat')\n",
      "Number of users still need to crawl: 0\n",
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/friends.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    }
   ],
   "source": [
    "verify_friends_dictionary()\n",
    "friends_dictionary = load_friends_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source(df, index):\n",
    "    time_created = df.loc[index, 'time']\n",
    "    try:\n",
    "        candidate = df[df.user_id.isin(df.loc[index, 'source_candidates'])].iloc[0]\n",
    "    except IndexError:\n",
    "        return np.NaN\n",
    "    if time_created > candidate.time:\n",
    "        return candidate.user_id\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "def find_root_and_generation(df, index):\n",
    "    row = df.iloc[index]\n",
    "    time = row.time\n",
    "    source = row.source_id\n",
    "    generation = int(0)\n",
    "    while ~np.isnan(source):\n",
    "        row = df[df.user_id == source].iloc[0]\n",
    "        source = row.source_id\n",
    "        generation += 1\n",
    "    root_time = row.time\n",
    "    return (row.user_id, generation, time-root_time)\n",
    "    \n",
    "    \n",
    "def find_network():\n",
    "    df = unique_users.copy()\n",
    "    df['source_id'] = [np.NaN for _ in range(len(df))]\n",
    "    for index in tqdm(reversed(range(len(df)))):\n",
    "        df.loc[index, 'source_id'] = find_source(unique_users, index)\n",
    "    df = df.loc[:, ['user', 'user_id', 'source_id', 'time', 'followers_count', 'friends_count']]\n",
    "    \n",
    "    df['seed_user_id'] = [np.NaN for _ in range(len(df))]\n",
    "    df['generation'] = [np.NaN for _ in range(len(df))]\n",
    "    df['time_since_seed'] = [np.NaN for _ in range(len(df))]\n",
    "    for index in tqdm(range(len(df))):\n",
    "        df.loc[index, 'seed_user_id'], df.loc[index, 'generation'], df.loc[index, 'time_since_seed']= find_root_and_generation(df, index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5011it [00:06, 788.72it/s]\n",
      "100%|██████████| 5011/5011 [00:12<00:00, 411.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping data to path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/network.dat\n",
      "('Finished dumping data to path '\n",
      " '/Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/network.dat')\n",
      "Loading data file from path /Users/lzhou/git/github/uclresearchanalysis/data/givenchy/pickle/network.dat\n",
      "'Loaded 5011 entires'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if calculate_network:\n",
    "    network = find_network()\n",
    "    network.head()\n",
    "    dump_network_dataframe(network)\n",
    "\n",
    "network = load_network_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_spreading_graph(df):\n",
    "    title = 'Number of new infected users during first 24 hours, for event {}'.format(project_name)\n",
    "    n_bins = math.floor(float(timeframe)/10)\n",
    "    sns.distplot(df['time'], kde=False, bins=n_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time (minutes) [10 minutes per bar]')\n",
    "    plt.ylabel('number of new infected users')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generation_vs_log_number_of_infected_users(df):\n",
    "    title = 'Generation vs number of infected users (log 10), for event {}'.format(project_name)\n",
    "#     n_bins = math.floor(float(timeframe)/10)\n",
    "    sns.distplot(df['generation'], kde=False, hist_kws={'log':True})\n",
    "    plt.title(title)\n",
    "    plt.xlabel('generation')\n",
    "    plt.ylabel('number of infected users (log 10)')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()\n",
    "    \n",
    "    print(df.groupby('generation').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_spreading_graph(df):\n",
    "    title = 'Total number of new infected users (log10) during first 24 hours, for event {}'.format(project_name)\n",
    "    number_of_seeds = 10\n",
    "    seed_count_series = df.seed_user_id.value_counts()\n",
    "    pallttes = sns.cubehelix_palette(number_of_seeds + 1)\n",
    "\n",
    "    draw_dataframe = df.copy()\n",
    "    draw_dataframe = draw_dataframe.filter(items=['time'])\n",
    "    draw_dataframe = draw_dataframe.reset_index()\n",
    "    draw_dataframe['counter'] = np.log10(range(1, len(draw_dataframe) + 1))\n",
    "    plt.plot(draw_dataframe['time'], draw_dataframe['counter'], marker='', color=pallttes[number_of_seeds], linewidth=1, alpha=0.9, label = 'all')\n",
    "\n",
    "    for index in range(number_of_seeds):\n",
    "        seed_user_id = seed_count_series.keys()[index]\n",
    "        data_to_plot = df.copy()\n",
    "        data_to_plot = data_to_plot[data_to_plot.seed_user_id == seed_user_id]['time']\n",
    "        data_to_plot = data_to_plot.reset_index()\n",
    "        data_to_plot['counter'] = np.log10(range(1, len(data_to_plot) + 1))\n",
    "        plt.plot(data_to_plot['time'], data_to_plot['counter'], marker='', color=pallttes[number_of_seeds - index], linewidth=1, alpha=0.9, label='Seed {}'.format(index + 1))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time t (minutes)')\n",
    "    plt.ylabel('total number of new infected users at t (log base 10)')\n",
    "    plt.legend()\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_after_spreading_vs_log_10_number_of_newly_infected_user(df):\n",
    "    title = 'time (minutes) since seed creation time vs newly infected user {}'.format(project_name)\n",
    "    number_of_seeds = 10\n",
    "    seed_count_series = df.seed_user_id.value_counts()\n",
    "    pallttes = sns.cubehelix_palette(number_of_seeds)\n",
    "    \n",
    "    for index in range(number_of_seeds):\n",
    "        seed_user_id = seed_count_series.keys()[index]\n",
    "        data_to_plot = df.copy()\n",
    "        data_to_plot = data_to_plot[data_to_plot.seed_user_id == seed_user_id]['time_since_seed']\n",
    "        data_to_plot = data_to_plot.reset_index()\n",
    "        data_to_plot['counter'] = np.log10(range(1, len(data_to_plot) + 1))\n",
    "        plt.plot(data_to_plot['time_since_seed'], data_to_plot['counter'], marker='', color=pallttes[number_of_seeds - index - 1], linewidth=1, alpha=0.9, label='Seed {}'.format(index + 1))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time (minutes)')\n",
    "    plt.ylabel('number of new infected users (log base 10)')\n",
    "    plt.legend()\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_vs_number_of_seeds():\n",
    "    title = 'Time vs Number of seeds, for event {}'.format(project_name)\n",
    "    seed_count_series = network.seed_user_id.value_counts()\n",
    "    seeds_dataframe = network[network.user_id.isin(seed_count_series.keys())].copy()\n",
    "    seeds_dataframe = order_and_reindex(seeds_dataframe, 'time')\n",
    "    n_bins = math.floor(float(timeframe)/10)\n",
    "    sns.distplot(seeds_dataframe['time'], kde=False, bins=n_bins, hist_kws={'log':True})\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time (minutes) [10 minutes per bar]')\n",
    "    plt.ylabel('number of seeds')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_vs_number_of_seeds():\n",
    "    title = 'Number of descendants(log 10) vs Number of seeds(log 10), for event {}'.format(project_name)\n",
    "    seed_count_series = network.seed_user_id.value_counts()\n",
    "    seeds_dataframe = network[network.user_id.isin(seed_count_series.keys())].copy()\n",
    "    seeds_dataframe = order_and_reindex(seeds_dataframe, 'time')\n",
    "    seeds_dataframe['y'] = 0\n",
    "    for index in range(len(seeds_dataframe)):\n",
    "        userid = seeds_dataframe.loc[index, 'user_id']\n",
    "        seeds_dataframe.loc[index, 'y'] = np.log10(seed_count_series.get(userid))\n",
    "    \n",
    "    n_bins = 20\n",
    "    \n",
    "    sns.distplot(seeds_dataframe['y'], kde=False, bins=n_bins, hist_kws={'log':True})\n",
    "    \n",
    "#     sns.distplot(seeds_dataframe['y'], kde=False, bins=n_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('number of descendants')\n",
    "    plt.ylabel('number of seeds')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_vs_log_seed_descedants():\n",
    "    title = 'Time vs Number of descendants (Log base 10) for event {}'.format(project_name)\n",
    "    seed_count_series = network.seed_user_id.value_counts()\n",
    "    seeds_dataframe = network[network.user_id.isin(seed_count_series.keys())].copy()\n",
    "    seeds_dataframe = order_and_reindex(seeds_dataframe, 'time')\n",
    "    seeds_dataframe['y'] = 0\n",
    "    for index in range(len(seeds_dataframe)):\n",
    "        userid = seeds_dataframe.loc[index, 'user_id']\n",
    "        seeds_dataframe.loc[index, 'y'] = np.log10(seed_count_series.get(userid))\n",
    "    seeds_dataframe.head()\n",
    "\n",
    "    # use the function regplot to make a scatterplot\n",
    "    sns.regplot(x=seeds_dataframe[\"time\"], y=seeds_dataframe['y'], fit_reg=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('time (minutes)')\n",
    "    plt.ylabel('number of descendants (log base 10)')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_followers_vs_log_seed_descedants():\n",
    "    title = 'Number of followers (Log base 10) vs Number of descendants (Log base 10) for event {}'.format(project_name)\n",
    "    seed_count_series = network.seed_user_id.value_counts()\n",
    "    seeds_dataframe = network[network.user_id.isin(seed_count_series.keys())].copy()\n",
    "    seeds_dataframe = order_and_reindex(seeds_dataframe, 'time')\n",
    "    seeds_dataframe['y'] = 0\n",
    "    for index in range(len(seeds_dataframe)):\n",
    "        userid = seeds_dataframe.loc[index, 'user_id']\n",
    "        seeds_dataframe.loc[index, 'y'] = seed_count_series.get(userid)\n",
    "    \n",
    "    # use the function regplot to make a scatterplot\n",
    "    sns.regplot(\n",
    "        x=np.log10(seeds_dataframe[\"followers_count\"]), \n",
    "        y=np.log10(seeds_dataframe['y']), \n",
    "        fit_reg=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('number of followers (log base 10)')\n",
    "    plt.ylabel('number of descendants (log base 10)')\n",
    "    plt.savefig('{}/{}'.format(config.settings['path']['result'], title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_of_top_10_seeds():\n",
    "    seed_count_series = network.seed_user_id.value_counts()\n",
    "    seeds_dataframe = network[network.user_id.isin(seed_count_series.keys())].copy()\n",
    "    seeds_dataframe = order_and_reindex(seeds_dataframe, 'time')\n",
    "    seeds_dataframe['number_of_descendants'] = 0\n",
    "    for index in range(len(seeds_dataframe)):\n",
    "        userid = seeds_dataframe.loc[index, 'user_id']\n",
    "        seeds_dataframe.loc[index, 'number_of_descendants'] = seed_count_series.get(userid)\n",
    "    seeds_dataframe = seeds_dataframe.sort_values(by='number_of_descendants', ascending=False)\n",
    "    seeds_dataframe = seeds_dataframe.set_index(np.arange(len(seeds_dataframe.index)))\n",
    "    seeds_dataframe = seeds_dataframe.loc[:10, ['user', 'time', 'followers_count', 'number_of_descendants']]\n",
    "    print(seeds_dataframe.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzhou/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user  user_id  source_id  time  followers_count  friends_count  \\\n",
      "generation                                                                   \n",
      "0.0          171      171          0   171              171            171   \n",
      "1.0         4839     4839       4839  4839             4839           4839   \n",
      "2.0            1        1          1     1                1              1   \n",
      "\n",
      "            seed_user_id  time_since_seed  \n",
      "generation                                 \n",
      "0.0                  171              171  \n",
      "1.0                 4839             4839  \n",
      "2.0                    1                1  \n",
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      "{} &             user &    time &  followers\\_count &  number\\_of\\_descendants \\\\\n",
      "\\midrule\n",
      "0  &       IAMFASHlON &   25.90 &           203931 &                   3323 \\\\\n",
      "1  &      MEENAVOGUEE &   91.42 &            55572 &                    727 \\\\\n",
      "2  &  supermodeldaiIy &  205.12 &            33363 &                    239 \\\\\n",
      "3  &  Fashionista\\_com &   17.00 &          2254291 &                    135 \\\\\n",
      "4  &       davelackie &    0.00 &           143119 &                    132 \\\\\n",
      "5  &        miuyorker &   83.35 &             9998 &                    112 \\\\\n",
      "6  &  harpersbazaarus &   72.85 &          1705680 &                     33 \\\\\n",
      "7  &   SteveKoehler22 &  146.95 &            17160 &                     23 \\\\\n",
      "8  &     InterviewMag &  140.83 &           301416 &                     16 \\\\\n",
      "9  &      TalkFilmSoc &   47.12 &             6281 &                     13 \\\\\n",
      "10 &       ninagarcia &  160.12 &          3364720 &                     11 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if calculate_analysis:\n",
    "    plt.rcParams['figure.figsize'] = [15, 9]\n",
    "    plot_hist_spreading_graph(network)\n",
    "    plot_generation_vs_log_number_of_infected_users(network)\n",
    "    plot_log_spreading_graph(network)    \n",
    "    plot_time_after_spreading_vs_log_10_number_of_newly_infected_user(network)\n",
    "    plot_time_vs_number_of_seeds()\n",
    "    plot_time_vs_number_of_seeds()\n",
    "    plot_time_vs_log_seed_descedants()\n",
    "    plot_followers_vs_log_seed_descedants()\n",
    "    get_details_of_top_10_seeds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
